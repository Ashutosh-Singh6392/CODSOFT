# -*- coding: utf-8 -*-
"""Credit_card_Fraud_Detection_Analysis

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qS0PtklRqNU3ps6OLITC0SOaxrbShU4k

**WorkFlow for this Assignment**

Credit card data ------> data Preprocessing ---> Data Analysis ----> Train_Test_Split ----->Logistinc_Regression_Model-----> Evaluate the model
"""



"""Work Flow for Credir Card fraud Analysis

Credit card data ------> data Preprocessing ---> Data Analysis ----> Train_Test_Split ----->Logistinc_Regression_Model-----> Evaluate the model
"""

from google.colab import drive

drive.mount('/content/drive')

import numpy as np                # importing numpy and pandas library for calculation and reading process
import pandas as pd

data = pd.read_csv('/creditcard.csv')     # with this command I am reading our credit card csv file

data.head()          # just checking file is uploaded correctly with top 5 record

from sklearn.model_selection import train_test_split

from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score

data.tail()    # it will give up last 5 record of our dataset.

#  dataset information

data.info()

data.shape

# checking the number of missing value in each column

data.isnull().sum()

# on some column we apply median to handle missing value


data['V17'].fillna(data[ 'V17'].median(),inplace=True)

data.isnull().sum()

data.fillna(1)

# another way to handle missing value

data.fillna({'V15':0, 'V18':0,'V19':0, 'V20':0, 'V21':0, 'V22':0, 'V23':0, 'V24':0, 'V25':0, 'V26':0, 'V27':0, 'V28':0},inplace=True)

# no missing value in this dataset


data.isnull().sum()

# distribution of legit transactions & fraudlent transactions


data['Class'].value_counts()

"""handle unbalanced data

level 0 = normal transactions
level 1 = fraudents transactions

"""

# storing the good data in anoter var and fraud data in another var

legal = data[data.Class==0]
fraud = data[data.Class==1]

legal.shape

fraud.shape

legal.Amount.describe()

"""Above command is showing all the statistical data of the transaction


"""

fraud.Amount.describe()

# camparing the value for legal and fraud transaction

data.groupby('Class').mean()

"""Under Sampling

maintaing the dataset for legal and fraud trabsactions

"""

legal_data = legal.Amount.describe()

legal_data = legal.sample(n=104)

"""Concatenating two DataFrames"""

ne_legal = pd.concat([legal_data, fraud], axis= 0)

ne_legal.head()

ne_legal.tail()

ne_legal['Class'].value_counts()

ne_legal.groupby('Class').mean()

"""Spilliting the data into features and target"""

X = ne_legal.drop(columns='Class', axis = 1)
Y = ne_legal['Class']

print(X)

print(Y)

"""Split the data into trainig data and testing data"""

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=1)

print(X.shape, X_train.shape, X_test.shape)

"""Model training"""

# LogisticRegression
model = LogisticRegression()

from sklearn.linear_model import LogisticRegression
logReg = LogisticRegression(solver='lbfgs', max_iter=3000)

# accuracy on training data

X_train_predicion =model.predict(X_train)

training_data_accuracy = accuracy_score(X_train_predicion, Y_train)

print("accuracy on traning data - " , training_data_accuracy)

# accuracy on test data

X_test_predicion = model.predict(X_test)

test_data_accuracy = accuracy_score(X_test_predicion, Y_test)

print('accuracy_od_testing :' , test_data_accuracy)





